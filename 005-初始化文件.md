## 系统属性设置

```bash
sudo yum install -y conntrack ntpdate ntp ipvsadm ipset jq iptabls curl sysstat libseccomp vim wget net-tools git -y
# 设置k8s 安装包源
cat <<EOF | sudo tee /etc/yum.repos.d/kubernetes.repo
[kubernetes]
name=Kubernetes
baseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-\$basearch
enabled=1
gpgcheck=0
repo_gpgcheck=0
gpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg
exclude=kubelet kubeadm kubectl
EOF
#设置系统时区为 服务器所在时区
sudo timedatectl set-timezone Asia/Tokyo
#将当前的UTC时间写入硬件时钟
sudo timedatectl set-local-rtc 0
#重启依赖于系统时间的服务
sudo systemctl restart rsyslog
sudo systemctl restart crond
#关闭系统不需要的服务
#postfix 邮件服务
sudo systemctl stop postfix
sudo systemctl disable postfix
sudo mkdir /var/log/journal
sudo mkdir /etc/systemd/journald.conf.d
cat <<EOF |sudo tee /etc/systemd/journald.conf.d/99-prophet.conf
[Journal]
# 持久化保存到磁盘
Storage =persistent
# 压缩历史日志
Compress=yes
SuncIntervalSec=5m
RateLimitInterval=30s
RateLimitBurst=1000
# 最大占用空间sudo systemctl restart docker
SystemMaxUse=10G
# 单个日志文件最大体积 200M
SystemMaxFileSize=200M
# 日志保存时间 2周
MaxRetentionSec=2week
# 不将日志转发到syslog
ForwardToSyslog=no
EOF
# 创建 Docker 配置文件目录
sudo mkdir /etc/docker
cat <<EOF | sudo tee /etc/docker/daemon.json
{
    "exec-opts": ["native.cgroupdriver=systemd"],
    "log-driver": "json-file",
    "log-opts": {
     "max-size": "100m"
    },
    "storage-driver": "overlay2"
}
EOF
sudo systemctl restart systemd-journald
sudo yum install -y yum-utils

sudo yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo
sudo setenforce 0
sudo sed -i 's/^SELINUX=enforcing$/SELINUX=permissive/' /etc/selinux/config
sudo swapoff -a 
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf  
net.bridge.bridge-nf-call-ip6tables=1
net.bridge.bridge-nf-call-iptables=1
net.ipv6.conf.all.disable_ipv6=1
net.ipv4.ip_forward=1
net.ipv4.tcp_tw_recycle=0
net.netfilter.nf_conntrack_max=2310720
vm.swappiness=0
vm.overcommit_memory=1
vm.panic_on_oom=0
fs.inotify-max_user_instances=8192
fs.inotify.max_user_watches=1048576
fs.file-max=52706963
fs.nr_open=52706963
EOF
cat <<EOF | sudo tee /etc/modules-load.d/containerd.conf
overlay
br_netfilter
EOF

# 手动开启允许检查桥接流量
sudo modprobe overlay
# 加载 br_netfilter 模块
sudo modprobe br_netfilter 
# 将参数写入到内核中
sudo sysctl --system
# 安装Docker
sudo yum install docker-ce docker-ce-cli containerd.io -y 
# 安装 1.19.16的 K8S
sudo yum install -y kubelet-1.19.16-0 kubectl-1.19.16-0 kubeadm-1.19.16-0 --disableexcludes=kubernetes
# 引导K8S Master 节点启动
sudo kubeadm init --kubernetes-version=1.19.16 --apiserver-advertise-address=10.10.2.44 --pod-network-cidr=10.244.0.0/16
# Docker 开机启动
sudo systemctl enable --now docker 
# kubelet K8s ApiServer 开机启动
sudo systemctl enable --now kubelet
# 设置不是root权限的用户可以直接使用kubectl命令
mkdir -p ~/.kube
sudo cp /etc/kubernetes/admin.conf ~/.kube/admin.conf
sudo chown $(id -u):$(id -g) ~/.kube/admin.conf
echo "KUBECONFIG=$HOME/.kube/admin.conf" >> ~/.bashrc
echo "KUBECONFIG=$HOME/.kube/admin.conf" >> ~/.bash_profile
source ~/.bashrc
source ~/.bash_profile
```





## 将节点加入到Master节点中

> **输出如下:**
>
> Your Kubernetes control-plane has initialized successfully!
>
> \#1 要使用您的集群,您需要使用普通用户执行以下命令
>
> To start using your cluster, you need to run the following as a regular user:
>
>  mkdir -p $HOME/.kube
>
>  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
>
>  sudo chown $(id -u):$(id -g) $HOME/.kube/config
>
> \#或者如果您是root用户,则可以运行
>
> Alternatively, if you are the root user, you can run:
>
>  export KUBECONFIG=/etc/kubernetes/admin.conf
>
> \#2 您现在应该为你的集群部署Pod网络,使用下列页面中的的列表部署一个合适的Pod网络
>
> You should now deploy a pod network to the cluster.
>
> Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
>
>  https://kubernetes.io/docs/concepts/cluster-administration/addons/
>
> \#3 然后您可以通过root身份,在每一个Node节点上运行以下命令来加入集群
>
> Then you can join any number of worker nodes by running the following on each as root:
>
> <a style="color:red;font-size:1.2em"> kubeadm join 192.168.2.11:6443 --token krkfy3.f8qxfwkzajrn4mq6 --discovery-token-ca-cert-hash sha256:5d2202d159614b9dca63131702c192afd9261d79270a5dce3fa18fb7d3f538d1</a>

红色显示的代码需要在work节点上面执行





## 验证集群

```
#查看所有nodes,状态都应该是Ready

kubectl get nodes
NAME    STATUS  ROLES  AGE  VERSION
k8s.node1  Ready  master  10m  v1.19.16
k8s.node2  Ready  <none>  72s  v1.19.16
k8s.node3  Ready  <none>  33s  v1.19.16

#查看所有pod —> 应该有等同于节点数量的 flannel pod 且状态为running

kubectl get pods --all-namespaces

**#测试网络可用性(非常重要)**

kubectl get pod --all-namespaces -o wide

网络之间不能通信,无非是三种现象
1 pod 不能ping 自己
2 pod 不能ping 宿主机 和 集群内其他宿主机ip
3 pod 不能ping 其他主机上的pod

验证方法:
找一个10网段的pod
ping 自身ip
ping 自身宿主机ip
ping 其他的集群节点主机ip  失败!
ping www.baidu.com -->  失败!

解决方案: 

**1 云安全组允许 所有流量 & 所有协议 & 所有端口范围 & 被K8S VPC内的节点访问 (有些请求不是TCP协议,这真坑!)**
第一步仍然不能解决,尝试下一步
**2 防火墙允许所有访问**
iptables -P INPUT ACCEPT
iptables -P FORWARD ACCEPT
iptables -F
iptables -L -n
```

**(可选操作) 重新生成令牌—后续节点加入集群中**

```

#默认情况下，令牌会在24小时后过期。如果要在当前令牌过期后将节点加入集群， 则可以通过在控制平面节点上运行以下命令来创建新令牌

kubeadm join --token <token> <control-plane-host>:<control-plane-port> --discovery-token-ca-cert-hash sha256:<hash>

<token> 通过命令进行刷新: kubeadm token create
<control-plane-host>:<control-plane-port>  是master节点的ip和端口 111.0.1.33:6443
<hash> 通过命令获取: 
```

**(可选操作)外部节点集群操作**

```
#将集群外部的机器加入到集群
scp root@<control-plane-host>:/etc/kubernetes/admin.conf .
kubectl --kubeconfig /etc/kubernetes/admin.conf get nodes

#建api服务器代理到本机
scp root@<control-plane-host>:/etc/kubernetes/admin.conf .
kubectl --kubeconfig ./admin.conf proxy

现在可以在本地访问API服务器 http://localhost:8001/api/v1
```

**重置kubeadmin**

```
遗憾的是,由于网络组件的复杂性和天朝网络的问题,安装网络组件时可能要重新编写kubeadm init 命令,所以需要先卸载kubeadmin

#每个节点执行一次
kubectl drain k8s.node1 --delete-emptydir-data --force --ignore-daemonsets
kubectl drain k8s.node2 --delete-emptydir-data --force --ignore-daemonsets
kubectl drain k8s.node3 --delete-emptydir-data --force --ignore-daemonsets

#每个节点上执行一次

kubeadm reset

#每个节点执行一次
kubectl delete k8s.node1
kubectl delete k8s.node2
kubectl delete k8s.node3
```

------







